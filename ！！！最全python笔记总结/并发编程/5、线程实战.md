# threading模块介绍
multiprocess模块的完全模仿了threading模块的接口，二者在使用层面，有很大的相似性，因而不再详细介绍。
# 开启线程的两种方式
方式一
```
from threading import Thread
import time
import  os

def work(name):
    print('%s%s开始工作' %(name,os.getpid()))
    print('%s%s完成工作' %(name,os.getpid()))

if __name__=='__main__':
    t=Thread(target=work,args=('alex',))
    t.start()
    print('主')
'''
alex11816开始工作
alex11816完成工作
主
'''
```
方式二
```
from threading import Thread
import time
import  os
class MyThread(Thread):
    def __init__(self,name):
        super().__init__()
        self.name=name
    def run(self):
        print('%s%s开始工作' %(self.name,os.getpid()))
        print('%s%s完成工作' %(self.name,os.getpid()))

if __name__=='__main__':
    t=MyThread('alex')
    t.start()
    print('主')
'''
alex21780开始工作
alex21780完成工作
主
'''
```
##### 线程的其他属性和方式
```
from threading import Thread,active_count,enumerate,current_thread
import os
import time
def work(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(2)
    print('%s%s is done '  %(name,os.getpid()))

if __name__=='__main__':
    t1 = Thread(target=work, args=('alex',))
    t1.start()
    t1.join() #原理和进程一样  主进程等待线程结束
    print(t1.is_alive())  #判断线程是否存活
    print(t1.getName()) #线程名字 Thread-1
    t1.setName('线程1') #设置线程的名字
    print(t1.getName()) #线程1
    print('主')
    print(t1.is_alive())


    print(active_count()) #活着的线程数目2  主线程加t1线程  如果开启了join就一个  join是主线程等待子线程结束，也就是只剩主线程了
    print(enumerate()) #当前活跃的线程  [<_MainThread(MainThread, started 42620)>, <Thread(Thread-1, started 39340)>]

    print(current_thread()) #<_MainThread(MainThread, started 35192)> 当前的线程对象
    print(current_thread().getName()) #MainThread 当前线程对象的名字  开启一个进程就会有一个主线程
    '''
    进程是资源单位，开启一个进程默认就会开启一个主线程
    主线程从某种意义上可以代替主进程，但是又不是主进程
    进程是资源单位，线程是执行单位
    '''
```
# 证明线程比进程快，线程的pid
同一个进程内的线程pid都是一样的
```
from threading import Thread
from multiprocessing import Process
import time
import os

def work(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(2)
    print('%s%s is done '  %(name,os.getpid()))

if __name__=='__main__':
    p1=Thread(target=work,args=('alex',))
    p2 = Thread(target=work, args=('egon',))
    p3 = Thread(target=work, args=('yuanhao',))
    p1.start()
    p2.start()
    p3.start()
    print('主')
'''
观察打印结果=pid都是一样的 线程共同创建它们的进程的地址空间
alex9224 is running
egon9224 is running
yuanhao9224 is running
主
egon9224 is done 
yuanhao9224 is done 
alex9224 is done 
'''
```
线程比进程开销小，速度快
```
from threading import Thread
from multiprocessing import Process
import time
import os

def work(name):
    print('%s%s is running' %(name,os.getpid()))
    print('%s%s is done '  %(name,os.getpid()))

if __name__=='__main__':
    p2=Process(target=work,args=('egon',))
    p1 = Thread(target=work, args=('alex',))
    p3 = Process(target=work, args=('egon',))
    p4 = Thread(target=work, args=('alex',))
    p2.start()
    p1.start()
    p3.start()
    p4.start()
    print('主')
'''
线程先结束 每创建一个进程都需要从新开辟内存空间
而线程是用创建它的进程的内存空间
alex16180 is running
alex16180 is done 
alex16180 is running
alex16180 is done 
主
egon34280 is running
egon34280 is done 
egon32116 is running
egon32116 is done 

'''
```
# 多线程并发的socket
 server端
 ```
 import socket
from threading import Thread
phone=socket.socket(socket.AF_INET,socket.SOCK_STREAM)
phone.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)
phone.bind(('127.0.0.1',8080))
phone.listen(5)

print('server starts...')

def work(conn):
    while True:
        try:
            data = conn.recv(1024)
            if not data:
                print('为啥没有数据呢')
                break
            print('客户端发过来的消息%s' % data)
            conn.send(data.upper())

        except  Exception as e:
            break
    conn.close()

if __name__=='__main__':
    while True:
        conn, client_addr = phone.accept()
        print(conn)
        t=Thread(target=work,args=(conn,))

        t.start()

phone.close()
 ```
 client端
 ```
 import socket

phone=socket.socket(socket.AF_INET,socket.SOCK_STREAM)

phone.connect(('127.0.0.1',8080))

while True:
    msg=input('>>:').strip()
    if not msg:continue
    phone.send(msg.encode('utf8'))
    data=phone.recv(1024).decode('gbk')
    print(data)

phone.close()
 ```
# 守护线程
### 守护进程的原理：
主进程的代码执行完毕，是代码执行完毕，那么此时主进程彻底死了吗，没有，
死了的话，子进程就成了僵尸进程了，主进程得等着子进程执行完毕回收
子进程的资源，所以在进程里，守护进程是等着主进程的代码执行完毕后就被干死了，仅仅是主进程的代码执行完毕后就被干死了

### 守护线程的原理：
主线程从某种意义上（执行意义上）可以代表主进程
守护线程是等待主线程执行完毕后，是主线程死的时候，不仅仅是代码执行完毕，的时候自己就被干死了，那么主线程什么时候死呢
主线程需要等到所有非守护线程执行完毕后，自己才死所以就是这样所有非守护线程死后，主线程死，最后守护线程死
### 示例
```
from threading import Thread
import os
import time
def work1(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(2)
    print('%s%s is done '  %(name,os.getpid()))

if __name__=='__main__':
    t1 = Thread(target=work1, args=('alex',))
    t1.daemon=True
    t1.start()
    print('主')
'''
alex12380 is running
主
#来不及打印done就结束了，因为这里除了主线程就是守护线程，主线程自己执行完毕后，没有任何
其他线程需要等待，所以直接就结束了，所以守护线程就被干死了
'''
```

```
from threading import Thread
import os
import time
def work1(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(2)
    print('%s%s is done '  %(name,os.getpid()))

def work2(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(5)
    print('%s%s is done '  %(name,os.getpid()))


if __name__=='__main__':
    t1 = Thread(target=work1, args=('alex',))
    t2 = Thread(target=work1, args=('egon',))
    t1.daemon=True
    t1.start()
    t2.start()
    print('主')
'''
alex30380 is running
egon30380 is running
主
alex30380 is done 
egon30380 is done 
# 守护线程执行完毕了  因为守护线程执行的快 执行完毕后，主线程依然再等t2-work2的执行还没结束

'''
```

```
from threading import Thread
import os
import time
def work1(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(5)
    print('%s%s is done '  %(name,os.getpid()))

def work2(name):
    print('%s%s is running' %(name,os.getpid()))
    time.sleep(2)
    print('%s%s is done '  %(name,os.getpid()))


if __name__=='__main__':
    t1 = Thread(target=work1, args=('alex',))
    t2 = Thread(target=work1, args=('egon',))
    t1.daemon=True
    t1.start()
    t2.start()
    print('主')
'''
alex37872 is running
egon37872 is running
主
egon37872 is done 
# 守护线程没有执行完毕就结束了  因为守护线程执行的慢 尚未执行完毕的时候，
#主线程包括主线程唯一需要等待的t2-work2都执行完毕了，
#此时主线程就执行完了，守护线程也直接死了，来不及执行了

'''
```
# GIL解释锁
GIL 保证的是 同一个进程里每次只有一个线程使用python解释器  是python解释器  gil是加在python解释器的锁


GIL是加在一个进程里的锁，锁住的python解释器，目的是让同一个进程里的多个线程串行运行


只保护python级别的数据  不保护程序员自己写的代码


因为GIL锁的机制 所以python的多线程并不能用上cpu的多核优势  但是python的多进程可以使用多核优势    一个进程里有多个线程，每个进程里的同时只能有一个线程去运行 那么多几个进程，比如五个进程的话，就同时有5个线程可以同时工作
### 证明GIL只是python级别的锁，不保护程序代码
```
from threading import Thread,Lock
import  time

n=100
def work():
    global n
    temp=n
    time.sleep(0.1)
    n=temp-1

if __name__=='__main__':
    l=[]
    start_time=time.time()
    for i in range(100):
        t=Thread(target=work)
        l.append(t)
        t.start()

    for t in l:
        t.join()
    print(n)
    print('run time:%s value:%s' %(time.time()-start_time,n))
'''
run time:0.11668872833251953 value:99
'''
每个线程都是进行减1的操作，这里有看不见的CTL锁，但是它保证的是在python解释器级别的一一执行，不保证程序员写的代码
所以现在的线程是并发执行，结果速度很快，值是100-1=99
```

```
from threading import Thread,Lock
import  time

n=100
def work():
    global n
    lock.acquire()
    temp=n
    time.sleep(0.1)
    n=temp-1
    lock.release()

if __name__=='__main__':
    lock=Lock()
    l=[]
    start_time=time.time()
    for i in range(100):
        t=Thread(target=work)
        l.append(t)
        t.start()

    for t in l:
        t.join()
    print('run time:%s value:%s' %(time.time()-start_time,n))
'''
run time:10.080582857131958 value:0
'''
想要保护代码级别的数据，就得自己处理锁了，线程里也有lock，加了lock后，线程一个一个执行，最终的结果是速度慢 值是0

```
### 计算密集型的程序，建议使用多进程，利用cpu多核优势，速度快
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

from threading import Thread
from multiprocessing import  Process
import  time
#计算密集型

def work():
    res=1
    for i in range(100000000):
        res+=i

if __name__=='__main__':
    p_l=[]
    start_time=time.time()

    for i in range(4): #cpu核数
        #p=Process(target=work)
        p = Thread(target=work)
        p_l.append(p)
        p.start()

    for p in p_l:
        p.join()

    print(time.time()-start_time)
    #12.340059280395508 进程
    #23.077324151992798 线程
'''
以上证明：
对于计算密集型的程序
使用多进程速度快

'''
```
### IO(网络IO/磁盘IO)密集型的程序，建议使用多线程
此时多核cpu对于io密集型程序，没有多大的作用，主要消耗的磁盘或者网络io，使用线程开销小
```
from threading import Thread
from multiprocessing import  Process
import  time
#IO密集型

def work():
    time.sleep(4)

if __name__=='__main__':
    p_l=[]
    start_time=time.time()

    for i in range(4): #cpu核数
        p=Process(target=work)
        #p = Thread(target=work)
        p_l.append(p)
        p.start()

    for p in p_l:
        p.join()

    print(time.time()-start_time)
    #进程 4.195133686065674
    #线程 4.002288579940796
```
# 死锁与递归锁
### 死锁是什么
解决死锁的方法是使用递归锁   
进程也有死锁与递归锁

所谓死锁： 是指两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程，如下就是死锁
```
from threading import Thread,Lock
import time
mutexA=Lock()
mutexB=Lock()

class MyThread(Thread):
    def run(self):
        self.func1()
        self.func2()
    def func1(self):
        mutexA.acquire()
        print('\033[41m%s 拿到A锁\033[0m' %self.name)

        mutexB.acquire()
        print('\033[42m%s 拿到B锁\033[0m' %self.name)
        mutexB.release()

        mutexA.release()

    def func2(self):
        mutexB.acquire()
        print('\033[43m%s 拿到B锁\033[0m' %self.name)
        time.sleep(2)

        mutexA.acquire()
        print('\033[44m%s 拿到A锁\033[0m' %self.name)
        mutexA.release()

        mutexB.release()

if __name__ == '__main__':
    for i in range(10):
        t=MyThread()
        t.start()

'''
Thread-1 拿到A锁
Thread-1 拿到B锁
Thread-1 拿到B锁
Thread-2 拿到A锁
然后就卡住，死锁了
'''
```
### 什么是递归锁
解决死锁的方法是使用递归锁   
在Python中为了支持在同一线程中多次请求同一资源，python提供了可重入锁RLock。  

这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。上面的例子如果使用RLock代替Lock，则不会发生死锁  ：
mutexA=mutexB=threading.RLock() 

一个线程拿到锁，counter加1,该线程内又碰到加锁的情况，则counter继续加1，这期间所有其他线程都只能等待，等待该线程释放所有锁，即counter递减到0为止  
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang
from threading import Thread,Lock,RLock
import time
mutex=RLock()

class MyThread(Thread):
    def run(self):
        self.func1()
        self.func2()
    def func1(self):
        mutex.acquire()
        print('\033[41m%s 拿到A锁\033[0m' %self.name)

        mutex.acquire()
        print('\033[42m%s 拿到B锁\033[0m' %self.name)
        mutex.release()

        mutex.release()

    def func2(self):
        mutex.acquire()
        print('\033[43m%s 拿到B锁\033[0m' %self.name)
        time.sleep(2)

        mutex.acquire()
        print('\033[44m%s 拿到A锁\033[0m' %self.name)
        mutex.release()

        mutex.release()

if __name__ == '__main__':
    for i in range(10):
        t=MyThread()
        t.start()
```
# 定时器
定时器，指定n秒后执行某操作
```
from threading import Timer


def hello():
    print("hello, world")

t = Timer(1, hello)
t.start()  # after 1 seconds, "hello, world" will be printed
```
验证码定时器
```
from threading import Timer
import random,time

class Code:
    def __init__(self):
        self.make_cache()

    def make_cache(self,interval=5):
        self.cache=self.make_code()
        print(self.cache)
        self.t=Timer(interval,self.make_cache)
        self.t.start()

    def make_code(self,n=4):
        res=''
        for i in range(n):
            s1=str(random.randint(0,9))
            s2=chr(random.randint(65,90))
            res+=random.choice([s1,s2])
        return res

    def check(self):
        while True:
            inp=input('>>: ').strip()
            if inp.upper() ==  self.cache:
                print('验证成功',end='\n')
                self.t.cancel()
                break


if __name__ == '__main__':
    obj=Code()
    obj.check()
```
# concurrent.futures模块 -进程池 线程池
concurrent.futures模块提供了高度封装的异步调用接口，使用这个模块可以很方便的是实现进程池和线程池，这也是以后的主流
### 基础用法
```
#1 介绍
concurrent.futures模块提供了高度封装的异步调用接口
ThreadPoolExecutor：线程池，提供异步调用
ProcessPoolExecutor: 进程池，提供异步调用
Both implement the same interface, which is defined by the abstract Executor class.

#2 基本方法
#submit(fn, *args, **kwargs)
异步提交任务

#map(func, *iterables, timeout=None, chunksize=1) 
取代for循环submit的操作

#shutdown(wait=True) 
相当于进程池的pool.close()+pool.join()操作
wait=True，等待池内所有任务执行完毕回收完资源后才继续
wait=False，立即返回，并不会等待池内的任务执行完毕
但不管wait参数为何值，整个程序都会等到所有任务执行完毕
submit和map必须在shutdown之前

#result(timeout=None)
取得结果

#add_done_callback(fn)
回调函数
```
### 进程池例子
```
from concurrent.futures import  ProcessPoolExecutor,ThreadPoolExecutor
import os,time,random

def work(n):
    print('%s is running' %os.getpid())
    time.sleep(random.randint(1,3))
    return  n**2

if __name__=='__main__':
    p=ProcessPoolExecutor() #不写默认是cpu的核数

    objs=[] #用来收集开启的对象进程的列表 因为是异步执行，一下子扔到进程池中，得把结果收集起来
    for i in  range(10): #循环开启10个进程
        obj = p.submit(work, i) ## 开启进程 (函数名，函数的参数)
        objs.append(obj)

    p.shutdown()#相当于之前学习的 p.close(),p.join() 确保所有任务执行完毕，暂时这一波不让再进进程

    #循环取出每个进程对象的执行结果
    for obj in objs:
        print(obj.result())
```
### 线程池例子
```
from concurrent.futures import  ProcessPoolExecutor,ThreadPoolExecutor
from threading import current_thread
import os,time,random

def work(n):
    print('%s is running' %current_thread().getName())
    return  n**2

if __name__=='__main__':
    p=ThreadPoolExecutor() #默认是开启 cpu核数*5 是cpu核数的5倍

    objs=[] #用来收集开启的对象线程的列表 因为是异步执行，一下子扔到线程池中，得把结果收集起来
    for i in  range(10): #循环开启10个线程
        obj = p.submit(work, i) ## 开启线程 (函数名，函数的参数)
        objs.append(obj)

    p.shutdown()

    #循环取出每个线程对象的执行结果
    for obj in objs:
        print(obj.result())
```
###　进程池回调函数例子
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

import requests
import os,time
from multiprocessing import Pool
from concurrent.futures import  ProcessPoolExecutor,ThreadPoolExecutor
def get_page(url):
    print('<%s> get :%s' %(os.getpid(),url))
    respone = requests.get(url)
    if respone.status_code == 200:
        return {'url':url,'text':respone.text}

def parse_page(dic):
    dic=dic.result() ##必须增加这个步骤 拿到结果  解释看下边
    print('<%s> parse :%s' %(os.getpid(),dic['url']))
    time.sleep(0.5)
    res='url:%s size:%s\n' %(dic['url'],len(dic['text'])) #模拟解析网页内容
    with open('db.txt','a') as f:
        f.write(res)


if __name__ == '__main__':
    p=ProcessPoolExecutor()
    urls = [
        'https://www.baidu.com',
        'https://www.python.org',
        'https://www.openstack.org',
        'https://help.github.com/',
        'http://www.sina.com.cn/'
    ]

    for url in urls:
        #p.apply_async(get_page,args=(url,),callback=parse_page)
        p.submit(get_page,url).add_done_callback(parse_page)
        '''
        callback回调函数
        运作原理：
        这里和前边进程池稍微不一样
        进程池是：
        前边的函数get_page执行完一个，就通知主进程，是通知主进程调用callback执行，并把 p.apply_async(get_page,args=(url,))得到的结果
        也就是自动调用了get方法拿到结果  传给回调函数
        现在的这个模块的原理是：
        这里不会自动调用result方法拿到结果给后边的回调函数，所以需要增加dic=dic.result() 这步，手动拿到结果
        
        针对此例的解释就是：
        N个url并发开始爬取内容(get_page函数的执行)，各自爬取各自的互不干扰，然后那个爬取完毕了，就通知主进程调用parse_page函数进行解析
        '''
    p.shutdown()
    print('主进程pid:',os.getpid())
'''
<8536> get :https://www.baidu.com
<12136> get :https://www.python.org
<32600> get :https://www.openstack.org
<17992> get :https://help.github.com/
<8536> get :http://www.sina.com.cn/
<18976> parse :https://www.baidu.com
<18976> parse :http://www.sina.com.cn/
<18976> parse :https://help.github.com/
<18976> parse :https://www.python.org
<18976> parse :https://www.openstack.org
主进程pid: 18976
'''
```
### 线程池回调函数例子
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

import requests
import os, time
from multiprocessing import Pool
from threading import current_thread
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor


def get_page(url):
    print('<%s> get :%s' % (current_thread().getName(), url))
    respone = requests.get(url)
    if respone.status_code == 200:
        return {'url': url, 'text': respone.text}


def parse_page(dic):
    dic = dic.result()  ##必须增加这个步骤 拿到结果  解释看下边
    print('<%s> parse :%s' % (current_thread().getName(), dic['url']))
    time.sleep(0.5)
    res = 'url:%s size:%s\n' % (dic['url'], len(dic['text']))  # 模拟解析网页内容
    with open('db.txt', 'a') as f:
        f.write(res)


if __name__ == '__main__':
    p = ThreadPoolExecutor()
    urls = [
        'https://www.baidu.com',
        'https://www.python.org',
        'https://www.openstack.org',
        'https://help.github.com/',
        'http://www.sina.com.cn/'
    ]

    for url in urls:
        # p.apply_async(get_page,args=(url,),callback=parse_page)
        p.submit(get_page, url).add_done_callback(parse_page)
        '''
        callback回调函数
        运作原理：
        这里和前边线程池稍微不一样
        线程池是：
        前边的函数get_page执行完一个，就通知主线程，是通知主线程调用callback执行，并把 p.apply_async(get_page,args=(url,))得到的结果
        也就是自动调用了get方法拿到结果  传给回调函数
        现在的这个模块的原理是：
        这里不会自动调用result方法拿到结果给后边的回调函数，所以需要增加dic=dic.result() 这步，手动拿到结果

        针对此例的解释就是：
        N个url并发开始爬取内容(get_page函数的执行)，各自爬取各自的互不干扰，然后那个爬取完毕了，就通知主线程调用parse_page函数进行解析
        '''
    p.shutdown()
    print('主线程pid:', current_thread().getName())
'''
<ThreadPoolExecutor-0_0> get :https://www.baidu.com
<ThreadPoolExecutor-0_1> get :https://www.python.org
<ThreadPoolExecutor-0_2> get :https://www.openstack.org
<ThreadPoolExecutor-0_3> get :https://help.github.com/
<ThreadPoolExecutor-0_4> get :http://www.sina.com.cn/
<ThreadPoolExecutor-0_0> parse :https://www.baidu.com
<ThreadPoolExecutor-0_4> parse :http://www.sina.com.cn/
<ThreadPoolExecutor-0_3> parse :https://help.github.com/
<ThreadPoolExecutor-0_2> parse :https://www.openstack.org
<ThreadPoolExecutor-0_1> parse :https://www.python.org
主线程pid: MainThread

'''
```