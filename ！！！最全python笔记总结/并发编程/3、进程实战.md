# multiprocessing模块介绍
python中的多线程无法利用多核优势，如果想要充分地使用多核CPU的资源（os.cpu_count()查看），在python中大部分情况需要使用多进程。Python提供了multiprocessing。 multiprocessing模块用来开启子进程，并在子进程中执行我们定制的任务（比如函数），该模块与多线程模块threading的编程接口类似。

multiprocessing模块的功能众多：支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件。

需要再次强调的一点是：与线程不同，进程没有任何共享状态，进程修改的数据，改动仅限于该进程内。
# 开启进程的两种方式
方式一

```
from multiprocessing import Process #Process是个类
import time
def work(name):
    print('%s is running' %name)
    time.sleep(2)
    print('%s is done ' %name)

if __name__=='__main__':
    p1=Process(target=work,args=('alex',))
    #p1 = Process(target=work, kwargs={'name':'alex'})
    '''
    p1就是一个进程对象
    target指定进程要做的事情，注意只写函数名，函数名
    args/kwargs是函数的参数，两种不同的形式，目的一样
    
    '''
    p1.start() #运行进程 开始执行任务
    print('主')

'''
打印结果：
主
alex is running
alex is done 
发现并没有按照代码的运行顺序打印，是先打印的’主‘这个字
因为 p1.start()只是去告诉系统我要开启一个进程了
至于开启进程，运行任务，结束任务都由操作系统来决定，程序是干涉不了的
所以程序告诉操作系统后，可以继续去干自己的事情了
'''
# 其他使用方法
print(p1.pid) #自己的pid
print(os.getpid()) #自己的pid
print(os.getppid()) #父亲的pid
p1.terminate() #不建议用  杀死p1 只会杀死p1 如果p1有子进程是不会杀死的，容易产生僵尸进程 不会立即杀死 平缓杀死
print(p1.is_alive()) #检测进程是否存活
p.daemon：默认值为False，如果设为True，代表p为后台运行的守护进程，当p的父进程终止时，p也随之终止，并且设定为True后，p不能创建自己的新进程，必须在p.start()之前设置
p.join([timeout]):主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join只能join住start开启的进程，而不能join住run开启的进程
```
方式二
```
from multiprocessing import Process
import time
class MyProcess(Process):
    def __init__(self,name):
        super().__init__()
        self.name=name
    def run(self):
        print('%s is running' % self.name)
        time.sleep(2)
        print('%s is done ' % self.name)

if __name__=='__main__':
    p1=MyProcess('alex')
    p1.start() #调动run函数执行
    print('主')
'''
主
alex is running
alex is done 
'''
```
# 多进程实现并发套接字通信
并发 并发的是服务端 一个服务端需要接收无数客户端的请求

先分析原版的socket服务端 
```
import socket
from multiprocessing  import  Process

phone=socket.socket(socket.AF_INET,socket.SOCK_STREAM)
phone.bind(('127.0.0.1',8080))
phone.listen(5)

while True:
    conn,clinet_addr=phone.accept()
    print(conn)
    while True:
        try:
            data=conn.recv(1024)
            if not data:break
            print(data)
            conn.send(data.upper())
        except Exception as e:
            break
    conn.close()


phone.close()

```
原版的socket想要达到并发的效果  
逻辑就是如下：  
来一个客户端1连接  服务端分一个子进程1   处理客户端1的请求   
来一个客户端2接  服务端分一个子进程2 处理客户端2的请求  
来一个客户端3连接  服务端分一个子进程3 处理客户端3的请求  

处理客户端请求的代码
```
 while True:
        try:
            data=conn.recv(1024)
            if not data:break
            print(data)
            conn.send(data.upper())
        except Exception as e:
            break
    conn.close()
```
然后再看实现方法
##### server端
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

import socket
from multiprocessing  import  Process

phone=socket.socket(socket.AF_INET,socket.SOCK_STREAM)
phone.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)
phone.bind(('127.0.0.1',8080))
phone.listen(5)

def work(conn,client_addr):  #将服务端需要做的任务 写成函数
    while True:
        try:
            data=conn.recv(1024)
            if not data:break
            print(data)
            conn.send(data.upper())
        except Exception as e:
            pass
    conn.close()

if __name__=='__main__':
    while True:
        conn, clinet_addr = phone.accept() #来一个连接  就开启一个p的子进程处理任务 子进程自己就开始处理了 然后phone.accept()又可以接收别的客户端了
        print(conn)
        p = Process(target=work, args=(conn, clinet_addr))
        p.start()
    phone.close()
```
##### client端
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

import socket

phone=socket.socket(socket.AF_INET,socket.SOCK_STREAM)
phone.connect(('127.0.0.1',8080))

while True:
    cmd=input('command>>:').strip()
    if not cmd:continue

    phone.send(cmd.encode('utf8'))
    data=phone.recv(1024).decode('gbk')
    print(data)

phone.close()
```
# Process的join方法
主进程等待子进程运行完毕后，再运行，是主进程在等子进程
```
from multiprocessing import Process
import time

def work(name):
    print('%s开始工作'%name)
    time.sleep(2)
    print('%s工作完毕'%name)

if __name__=='__main__':
    p1=Process(target=work,args=('alex',))
    p1.start()
    p1.join() #主进程等待子进程p1运行完毕后再运行
    #p1.join(1) #指定等到秒数
    print('主')
#打印结果
alex开始工作
alex工作完毕
主

'''
场景：
需要得到子进程的一个运行结果 然后在做操作
所以主进程要等着子进程运行完毕 拿到想要的东西
'''
```
深入剖析
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang


from multiprocessing import Process
import time

def work1(name):
    print('%s开始工作'%name)
    time.sleep(2)
    print('%s工作完毕'%name)

def work2(name):
    print('%s开始工作'%name)
    time.sleep(3)
    print('%s工作完毕'%name)


def work3(name):
    print('%s开始工作'%name)
    time.sleep(5)
    print('%s工作完毕'%name)

if __name__=='__main__':

    p1=Process(target=work1,args=('alex',))
    p2 = Process(target=work2, args=('egon',))
    p3 = Process(target=work3, args=('qiji',))

    start_time = time.time()
    p1.start() #开启一个子进程自己工作去了
    p2.start() #也开启一个子进程自己工作去了
    p3.start() #也开启一个子进程自己工作去了  三个进程是基本是同时执行的，自己玩自己的，是并发的效果

    p1.join()
    p2.join()
    p3.join() #是主进程等待子进程执行完毕，整个过程并不会影响子进程自己的执行 主进程在等p3的时候，其实前两个已经执行完毕了
    
    stop_time=time.time()
    used_time=stop_time-start_time
    print('总用时间为%s' %used_time)
    print('主')
'''
alex开始工作
egon开始工作
qiji开始工作
alex工作完毕
egon工作完毕
qiji工作完毕
总用时间为5.154915809631348  #发现最后的所用的时间基本是用时最长的任务  并不是所有任务的时间和
主
'''
```
#  僵尸进程和孤儿进程
https://www.cnblogs.com/Anker/p/3271773.html
# 守护进程
主进程创建守护进程   
　　其一：守护进程会在主进程代码执行结束后就终止 是代码运行完毕  
　　其二：守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children  
注意：进程之间是互相独立的，主进程代码运行结束，守护进程随即终止  
```
from multiprocessing import Process
import time
import  os

def work1(name):
    print('%s开始工作'%name)
    time.sleep(2)
    print('%s工作完毕'%name)

if __name__=='__main__':

    p1=Process(target=work1,args=('alex',))
    p1.daemon=True  #设置为守护进程
    p1.start()
    print('主')
'''
主 #打印完 ‘主’ 字后，主进程就结束了，主进程结束后,因为work函数要执行时间
比较长，所以来不及打印就死了，如果p1执行足够快的话，有时候也是能看到的，但是
原理始终是主进程的代码执行完毕，是代码执行完毕，那么此时主进程彻底死了吗，没有，
死了的话，子进程就成了僵尸进程了，主进程得等着子进程执行完毕回收
子进程的资源，所以在进程里，守护进程是等着主进程的代码执行完毕后就被干死了
这里和线程后区别，后续讲
'''
```
```
from multiprocessing import Process
import time
import random

class Piao(Process):
    def __init__(self,name):
        self.name=name
        super().__init__()
    def run(self):
        print('%s is piaoing' %self.name)
        time.sleep(random.randrange(1,3))
        print('%s is piao end' %self.name)


p=Piao('egon')
p.daemon=True #一定要在p.start()前设置,设置p为守护进程,禁止p创建子进程,并且父进程代码执行结束,p即终止运行
p.start()
print('主')
```
# 进程同步锁
先看一个效果理解一下锁的概念
```
from multiprocessing import Process,Lock #导入Lock模块
import time
import  os

def work1(lock,name):
    lock.acquire()  #加锁
    print('%s开始工作'%name)
    time.sleep(2)
    print('%s工作完毕'%name)
    lock.release() #解锁

if __name__=='__main__':
    lock=Lock() #实例化一个lock对象出来

    p1=Process(target=work1,args=(lock,'alex',))
    p2 = Process(target=work1, args=(lock,'egon',))
    p1.start()
    p2.start()
    print('主')
'''
主
alex开始工作
alex工作完毕
egon开始工作
egon工作完毕
'''
```
并发的效率虽然高，但是竞争难免带来错乱，锁会让并行变成串行，牺牲了效率，提高了准确性

##### 模拟买票的过程
```
#文件db的内容为：{"count":1}
#注意一定要用双引号，不然json无法识别
from multiprocessing import Process,Lock
import time,json,random
def search():
    dic=json.load(open('db.txt'))
    print('\033[43m剩余票数%s\033[0m' %dic['count'])

def get():
    dic=json.load(open('db.txt'))
    time.sleep(0.1) #模拟读数据的网络延迟
    if dic['count'] >0:
        dic['count']-=1
        time.sleep(0.2) #模拟写数据的网络延迟
        json.dump(dic,open('db.txt','w'))
        print('\033[43m购票成功\033[0m')

def task(lock):
    search()
    lock.acquire()
    get()
    lock.release()
if __name__ == '__main__':
    lock=Lock()
    for i in range(100): #模拟并发100个客户端抢票
        p=Process(target=task,args=(lock,))
        p.start()
```
##### 总结
加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全。

虽然可以用文件共享数据实现进程间通信，但问题是:   
1.效率低（共享数据基于文件，而文件是硬盘上的数据）   
2.需要自己加锁处理   

因此我们最好找寻一种解决方案能够兼顾：  
1、效率高（多个进程共享一块内存的数据）  
2、帮我们处理好锁问题。这就是mutiprocessing模块为我们提供的基于消息的IPC通信机制：队列和管道。  

队列和管道都是将数据存放于内存中

队列又是基于（管道+锁）实现的，可以让我们从复杂的锁问题中解脱出来，

我们应该尽量避免使用共享数据，尽可能使用消息传递和队列，避免处理复杂的同步和锁问题，而且在进程数目增多时，往往可以获得更好的可获展性。
# 队列
先进先出   

进程之间的通信，共享数据存储在内存中，还自己处理了锁（rabbitmq就是这个理念，还提供了存储）   

进程彼此之间互相隔离，要实现进程间通信（IPC），multiprocessing模块支持两种形式：队列和管道，这两种方式都是使用消息传递的。队列的底层就是使用管道和锁实现的

##### 队列的基础用法
```
from multiprocessing import Queue

q=Queue(3)  #指定队列的长度为3 规定了能放几个就能放几个，多放就会卡主

#往队列里放东西 放啥都可以
q.put('first')
q.put('second')
q.put('third')

print(q.get())
print(q.get())
print(q.get())

q.put('fouth')
print(q.get()) #放满了

'''
first
second
third
fouth
'''
```
##### 其他方法
```
Queue([maxsize]):创建共享的进程队列，Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递。maxsize是队列中允许最大项数，省略则无大小限制。

q.put方法用以插入数据到队列中，put方法还有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，该方法会阻塞timeout指定的时间，直到该队列有剩余的空间。如果超时，会抛出Queue.Full异常。如果blocked为False，但该Queue已满，会立即抛出Queue.Full异常。
2 q.get方法可以从队列读取并且删除一个元素。同样，get方法有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，那么在等待时间内没有取到任何元素，会抛出Queue.Empty异常。如果blocked为False，有两种情况存在，如果Queue有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出Queue.Empty异常.
3  
4 q.get_nowait():同q.get(False)
5 q.put_nowait():同q.put(False)
6 
7 q.empty():调用此方法时q为空则返回True，该结果不可靠，比如在返回True的过程中，如果队列中又加入了项目。
8 q.full()：调用此方法时q已满则返回True，该结果不可靠，比如在返回True的过程中，如果队列中的项目被取走。
9 q.qsize():返回队列中目前项目的正确数量，结果也不可靠，理由同q.empty()和q.full()一样

 q.cancel_join_thread():不会在进程退出时自动连接后台线程。可以防止join_thread()方法阻塞
2 q.close():关闭队列，防止队列中加入更多数据。调用此方法，后台线程将继续写入那些已经入队列但尚未写入的数据，但将在此方法完成时马上关闭。如果q被垃圾收集，将调用此方法。关闭队列不会在队列使用者中产生任何类型的数据结束信号或异常。例如，如果某个使用者正在被阻塞在get()操作上，关闭生产者中的队列不会导致get()方法返回错误。
3 q.join_thread()：连接队列的后台线程。此方法用于在调用q.close()方法之后，等待所有队列项被消耗。默认情况下，此方法由不是q的原始创建者的所有进程调用。调用q.cancel_join_thread方法可以禁止这种行为
```

# 生产者和消费者-队列应用
基于队列做生产者和消费者模型
### 模式说明
##### 生产者消费者模型
在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。
##### 为什么要使用生产者和消费者模式
在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。
##### 什么是生产者消费者模式
生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。

##### 生产者消费者模型总结
```
#程序中有两类角色
    一类负责生产数据（生产者）
    一类负责处理数据（消费者）
        
#引入生产者消费者模型为了解决的问题是：
    平衡生产者与消费者之间的工作能力，从而提高程序整体处理数据的速度
        
#如何实现：
    生产者<-->队列<——>消费者
```
##### 示例
##### 生产者消费者模型实现类程序的解耦和
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang
from multiprocessing import Queue,Process
import time
import os

def producer(q):
    for i in range(5):
        time.sleep(2)
        res='包子%s' %i
        q.put(res)
        print('%s生产%s' %(os.getpid(),res))

def consumer(q):
    while True:
        res=q.get()
        time.sleep(2)
        print('%s吃%s' %(os.getpid(),res)

if __name__=='__main__':
    q=Queue()
    #生产者
    p1=Process(target=producer,args=(q,))
    #消费者
    c1=Process(target=consumer,args=(q,))

    p1.start()
    c1.start()
    print('主')
    '''
    主
14452生产包子0
15168吃包子0
14452生产包子1
15168吃包子1
14452生产包子2
15168吃包子2
14452生产包子3
15168吃包子3
14452生产包子4
15168吃包子4
    '''
```
此时的问题是主进程永远不会结束，原因是：生产者p在生产完后就结束了，但是消费者c在取空了q之后，则一直处于死循环中且卡在q.get()这一步。

解决方式无非是让生产者在生产完毕后，往队列中再发一个结束信号，这样消费者在接收到结束信号后就可以，是消费者还在等待
##### 解决一个生产者和一个消费者
```
from multiprocessing import Process,JoinableQueue
import time
import os

def producer(q):
    for i in range(5):
        time.sleep(2)
        res='包子%s' %i
        q.put(res)
        print('%s生产%s' %(os.getpid(),res))
    q.join() #记录消费者消费次数 等着消费者都消费完

def consumer(q):
    while True:
        res=q.get()
        time.sleep(2)
        print('%s吃%s' %(os.getpid(),res))
        q.task_done() #消费者消费一条  就告诉生产者


if __name__=='__main__':
    q=JoinableQueue()
    #生产者
    p1=Process(target=producer,args=(q,))
    #消费者
    c1=Process(target=consumer,args=(q,))

    c1.daemon=True #主进程结束了，肯定也消费完了，所以也随之结束吧
    p1.start()
    c1.start()
    p1.join() #主进程在等p1结束，p1在等消费者结束
    print('主')
```
##### 多个生产者和消费者解决方法
```
from multiprocessing import Process,JoinableQueue
import time
import os

def producer(q):
    for i in range(5):
        time.sleep(2)
        res='包子%s' %i
        q.put(res)
        print('%s生产%s' %(os.getpid(),res))
    q.join() #记录消费者消费次数 等着消费者都消费完

def consumer(q):
    while True:
        res=q.get()
        time.sleep(2)
        print('%s吃%s' %(os.getpid(),res))
        q.task_done() #消费者消费一条  就告诉生产者


if __name__=='__main__':
    q=JoinableQueue()
    #生产者
    p1=Process(target=producer,args=(q,))
    p2 = Process(target=producer, args=(q,))
    #消费者
    c1=Process(target=consumer,args=(q,))
    c2 = Process(target=consumer, args=(q,))

    c1.daemon=True #主进程结束了，肯定也消费完了，所以也随之结束吧
    c2.daemon = True
    p1.start()
    p2.start()
    c1.start()
    c2.start()
    p1.join() #主进程在等p1结束，p1在等消费者结束
    p2.join()



    print('主')
```
# 进程池
在利用Python进行系统管理的时候，特别是同时操作多个文件目录，或者远程控制多台主机，并行操作可以节约大量的时间。多进程是实现并发的手段之一，需要注意的问题是：

1、很明显需要并发执行的任务通常要远大于核数

2、一个操作系统不可能无限开启进程，通常有几个核就开几个进程


3、进程开启过多，效率反而会下降（开启进程是需要占用系统资源的，而且开启多余核数目的进程也无法做到并行

例如当被操作对象数目不大时，可以直接利用multiprocessing中的Process动态成生多个进程，十几个还好，但如果是上百个，上千个。。。手动的去限制进程数量却又太过繁琐，此时可以发挥进程池的功效。
我们就可以通过维护一个进程池来控制进程数目，比如httpd的进程模式，规定最小进程数和最大进程数...

ps：对于远程过程调用的高级应用程序而言，应该使用进程池，Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，就重用进程池中的进程。

### 同步调用 apply
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang
from multiprocessing import   Pool
import os,time

def work(name):
    print('%s%s开始工作了' %(os.getpid(),name))
    time.sleep(2)
    print('%s%s工作完毕' %(os.getpid(),name))
    return  'ok'

if __name__=='__main__':
    p=Pool(4) #进程池中从无到有的创建3个进程，以后就是这三个进程在执行任务 不写默认是os.cpu_count()
    for i in range(8):
        res=p.apply(work,args=('alex',)) #同步提交任务 串行工作 拿到结果再往下走
        print(res)

'''
#观察打印结果 发现进程的pid在复用 总是408 13260 2904 3668 
408alex开始工作了
408alex工作完毕
ok
13260alex开始工作了
13260alex工作完毕
ok
2904alex开始工作了
2904alex工作完毕
ok
3668alex开始工作了
3668alex工作完毕
ok
408alex开始工作了
408alex工作完毕
ok
13260alex开始工作了
13260alex工作完毕
ok
2904alex开始工作了
2904alex工作完毕
ok
3668alex开始工作了
3668alex工作完毕
ok
'''
```
### 异步调用 apply_async
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

from multiprocessing import   Pool
import os,time

def work(name):
    print('%s%s开始工作了' %(os.getpid(),name))
    time.sleep(2)
    print('%s%s工作完毕' %(os.getpid(),name))
    #return  'ok'

if __name__=='__main__':
    p=Pool()
    res_l=[]
    for i in range(8):
        res=p.apply_async(work,args=('alex',)) #异步提交任务 直接将所有任务扔到进程池中不等待结果
        res_l.append(res) #收集任务 同时任务也在执行

    p.close() #确保这一波为未执行完毕前 别的进程无法进入进程池 否则如果源源不断的进入新的任务 那里边的任务永远的执行不完了
    p.join() #等待所有任务都执行完毕 确保主进程结束前所有任务都执行完毕  是主进程结束前  如果主进程一直不结束 这个也就没什么用了

    for r in res_l: #循环任务结果
        #print(r.get())  #并统一展示 打印才会才会显示返回值 没有返回值就不用打印
        r.get()

'''
12156alex开始工作了
12828alex开始工作了
15244alex开始工作了
2572alex开始工作了
12156alex工作完毕
12156alex开始工作了
12828alex工作完毕
12828alex开始工作了
15244alex工作完毕
15244alex开始工作了
2572alex工作完毕
2572alex开始工作了
12156alex工作完毕
12828alex工作完毕
15244alex工作完毕
2572alex工作完毕
ok
ok
ok
ok
ok
ok
ok
ok

'''
```
### 同步和异步对比
##### 使用进程池（异步调用,apply_async）
```
#coding: utf-8
from multiprocessing import Process,Pool
import time

def func(msg):
    print( "msg:", msg)
    time.sleep(1)
    return msg

if __name__ == "__main__":
    pool = Pool(processes = 3)
    res_l=[]
    for i in range(10):
        msg = "hello %d" %(i)
        res=pool.apply_async(func, (msg, ))   #维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去
        res_l.append(res)
    print("==============================>") #没有后面的join，或get，则程序整体结束，进程池中的任务还没来得及全部执行完也都跟着主进程一起结束了

    pool.close() #关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成
    pool.join()   #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束

    print(res_l) #看到的是<multiprocessing.pool.ApplyResult object at 0x10357c4e0>对象组成的列表,而非最终的结果,但这一步是在join后执行的,证明结果已经计算完毕,剩下的事情就是调用每个对象下的get方法去获取结果
    for i in res_l:
        print(i.get()) #使用get来获取apply_aync的结果,如果是apply,则没有get方法,因为apply是同步执行,立刻获取结果,也根本无需get
```
##### 使用进程池（同步调用,apply）
```
#coding: utf-8
from multiprocessing import Process,Pool
import time

def func(msg):
    print( "msg:", msg)
    time.sleep(0.1)
    return msg

if __name__ == "__main__":
    pool = Pool(processes = 3)
    res_l=[]
    for i in range(10):
        msg = "hello %d" %(i)
        res=pool.apply(func, (msg, ))   #维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去
        res_l.append(res) #同步执行，即执行完一个拿到结果，再去执行另外一个
    print("==============================>")
    pool.close()
    pool.join()   #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束

    print(res_l) #看到的就是最终的结果组成的列表
    for i in res_l: #apply是同步的，所以直接得到结果，没有get()方法
        print(i)

```
### 使用进程池实现socket通信
##### server端
```
Pool内的进程数默认是cpu核数，假设为4（查看方法os.cpu_count()）
#开启6个客户端，会发现2个客户端处于等待状态
#在每个进程内查看pid，会发现pid使用为4个，即多个客户端公用4个进程
from socket import *
from multiprocessing import Pool
import os

server=socket(AF_INET,SOCK_STREAM)
server.setsockopt(SOL_SOCKET,SO_REUSEADDR,1)
server.bind(('127.0.0.1',8080))
server.listen(5)

def talk(conn,client_addr):
    print('进程pid: %s' %os.getpid())
    while True:
        try:
            msg=conn.recv(1024)
            if not msg:break
            conn.send(msg.upper())
        except Exception:
            break

if __name__ == '__main__':
    p=Pool()
    while True:
        conn,client_addr=server.accept()
        p.apply_async(talk,args=(conn,client_addr))
        # p.apply(talk,args=(conn,client_addr)) #同步的话，则同一时间只有一个客户端能访问 因为是串行运行，完事一个才能进行下一个
```
##### client端
```
from socket import *

client=socket(AF_INET,SOCK_STREAM)
client.connect(('127.0.0.1',8080))

while True:
    msg=input('>>: ').strip()
    if not msg:continue

    client.send(msg.encode('utf-8'))
    msg=client.recv(1024)
    print(msg.decode('utf-8'))
```
### 回调函数
##### 场景
进程池中任何一个任务一旦处理完了，就立即告知主进程：我好了额，你可以处理我的结果了。主进程则调用一个函数去处理该结果，该函数即回调函数

我们可以把耗时间（阻塞）的任务放到进程池中，然后指定回调函数（主进程负责执行），这样主进程在执行回调函数时就省去了I/O的过程，直接拿到的是任务的结果。
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Author qiqiYang

import requests #pip3 install requests
import os,time
from multiprocessing import Pool
def get_page(url):
    print('<%s> get :%s' %(os.getpid(),url))
    respone = requests.get(url)
    if respone.status_code == 200:
        return {'url':url,'text':respone.text}

def parse_page(dic):
    print('<%s> parse :%s' %(os.getpid(),dic['url']))
    time.sleep(0.5)
    res='url:%s size:%s\n' %(dic['url'],len(dic['text'])) #模拟解析网页内容
    with open('db.txt','a') as f:
        f.write(res)


if __name__ == '__main__':
    p=Pool(4)
    urls = [
        'https://www.baidu.com',
        'https://www.python.org',
        'https://www.openstack.org',
        'https://help.github.com/',
        'http://www.sina.com.cn/'
    ]

    for url in urls:
        p.apply_async(get_page,args=(url,),callback=parse_page)
        '''
        callback回调函数
        运作原理：
        前边的函数get_page执行完一个，就通知主进程，是通知主进程调用callback执行，并把 p.apply_async(get_page,args=(url,))得到的结果传给回调函数
        针对此例的解释就是：
        N个url并发开始爬取内容(get_page函数的执行)，各自爬取各自的互不干扰，然后那个爬取完毕了，就通知主进程调用parse_page函数进行解析
        '''

    p.close()  #这两行必须有
    p.join()
    print('主进程pid:',os.getpid())
    '''
<18428> get :https://www.baidu.com
<18244> get :https://www.python.org
<43264> get :https://www.openstack.org
<18320> get :https://help.github.com/
<18428> get :http://www.sina.com.cn/
<45488> parse :https://www.baidu.com
<45488> parse :http://www.sina.com.cn/
<45488> parse :https://help.github.com/
<45488> parse :https://www.openstack.org
<45488> parse :https://www.python.org
主进程pid: 45488 #根据pid发现是主进程在做回调函数的工作
    '''
```
# map用法
```
from concurrent.futures import  ProcessPoolExecutor,ThreadPoolExecutor
import os,time,random

def work(n):
    print('%s is running' %os.getpid())
    time.sleep(random.randint(1,3))
    return  n**2

if __name__=='__main__':
    p=ProcessPoolExecutor() #不写默认是cpu的核数

    # objs=[]
    # for i in  range(10):
    #     obj = p.submit(work, i)
    #     objs.append(obj)
    #
    # p.shutdown()
    #
    # for obj in objs:
    #     print(obj.result())


    #这三行代码等同于上边注释的代码意思
    obj=p.map(work,range(10))
    p.shutdown()
    print(list(obj)) #[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
```